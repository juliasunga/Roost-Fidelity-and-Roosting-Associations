---
title: "RoostFidelitySampleCode"
author: "JuliaSunga"
date: "21/03/2022"
output: pdf_document
---


For all analyses, read in RoostUseData2012-2019.csv then subset to desired year. Code is also provided to generate the bar plot of the number of bats in each year as seen in Figure 2.

After this first chunk is run, should be able to run any other chunk independently so there is some repetition between sections.  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table)

roostusedata<-as.data.table(read.csv("RoostData2012-2019.csv"))
mysubset<-roostusedata[reader_year== "2012"]


#Figure 2 histogram code 

#group by individual and year
library(tidyr)
library(dplyr)

#drop 2012
roostusedata2<-roostusedata %>% filter(!(reader_year=="2012"))


#for each bat, create only 1 row of data for each year in which they appear
roostusedata2 %>% dplyr::group_by(pit, reader_year) %>% dplyr::summarise()->yearpresence

#count the number of years in which a bat appears
yearpresence %>% count(pit) %>% ungroup() ->yearcount

#rename the count column
yearcount$count2<-yearcount$n

#count the number of bats that each have a certain number of years in which they are included (ie. number of bats that are in 3 years, 4 years, etc)
count2<-yearcount %>% count(count2, name="countcount")

#generate barplot
barplot(count2$countcount~count2$count2, ylim=c(0,200), ylab="Number of Individuals", xlab="Number of Years in Which an Individual Appears")

```

For the full year networks, this code is used to create barplots of the most used box by individual bats, separated based on their assigned subgroup. (Figure 3). Note that in some years, community numbers were swapped to allow colours to be relatively consistent between areas but this did not change the values of the plots or community assignments. 

```{r}
library(ggplot2)
library(berryFunctions)
library(dplyr)
library(data.table)
library(igraph)
library(asnipe)

#get most used box for each individual and merge with this group information
DT<-data.table(mysubset)
DT$box1<-as.numeric(DT$box)
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}
boxuse<-DT[,.(boxfreq=getmode(box1)), by=pit]

#assign community membership as a variable (need to create the network to do this)
mysubset$daygroup<-paste(mysubset$box, "-", mysubset$reader_date)

individuals <- data.frame(ID=c(mysubset$pit),group=c(mysubset$daygroup),day=c(mysubset$reader_date))
sampleGBI <- get_group_by_individual(individuals, data_format = "individuals")
net_matrix <- get_network(sampleGBI,data_format="GBI",association_index="SRI")
graph_object <- graph.adjacency(net_matrix,mode="undirected",diag=FALSE,weighted=TRUE)
community.observed <- fastgreedy.community(graph.adjacency(net_matrix, mode="undirected",weighted=TRUE))
boxuse$group<-membership(community.observed)

#swap 2013 to match colours
boxuse$group[boxuse$group==2]<-0
boxuse$group[boxuse$group==3]<-2
boxuse$group[boxuse$group==0]<-3
#summarize # of individuals of one box in each assigned group
plotready<-boxuse %>% group_by(boxfreq, group) %>% summarise(tot=n())

#set boxfreq and group to a factors
plotready$boxfreq<-as.character(plotready$boxfreq)
plotready$boxfreq<-as.factor(plotready$boxfreq)
plotready$group<-as.factor(plotready$group)

#ensure all boxes are included - add rows for 0 values which would not regularly be there
dat2 <- with(plotready, expand.grid(group = levels(group), boxfreq = levels(boxfreq)))
dat3<-addRows(dat2, 1, values=c(NA, "101"))
dat3<-addRows(dat3, 1, values=c(NA, "102"))
dat3<-addRows(dat3, 1, values=c(NA, "103"))
dat3<-addRows(dat3, 1, values=c(NA, "104"))
dat3<-addRows(dat3, 1, values=c(NA, "105"))
dat3<-addRows(dat3, 1, values=c(NA, "106"))
dat3<-addRows(dat3, 1, values=c(NA, "107"))
dat3<-addRows(dat3, 1, values=c(NA, "108"))
dat3<-addRows(dat3, 1, values=c(NA, "109"))
dat3<-addRows(dat3, 1, values=c(NA, "110"))
dat3<-addRows(dat3, 1, values=c(NA, "111"))

#add this into the plotready data to fill in blanks
newdat <- merge(plotready, dat3, all.y = TRUE)
newdat$v1[is.na(newdat$tot)] <- 0

#reorder boxes to correspond to general geographic position (HB on the West side so left side of plot)
newdat$boxfreq<-factor(newdat$boxfreq, levels= c("109", "110", "111",  "101", "102", "103", "104", "105", "106", "107", "108"))

#generate grouped plot
p<-ggplot(newdat, aes(fill=group, y=tot, x=boxfreq)) + 
  geom_bar(position= "dodge",stat="identity") 
 
p + scale_x_discrete(drop=FALSE)+labs(fill="Subgroup", x = "\n\nMost Frequently Used Roost Box", y = "# of Individuals") +theme_classic() + theme(axis.text=element_text(size=16)) + theme(axis.title=element_text(size=20)) +  scale_fill_manual(values=c(categorical_pal(4)), na.translate=FALSE) +theme(legend.position="none")

```


Within each year, then want to separate the individuals by their most used roosting location. 

```{r}
library(data.table)
library(dplyr)
#convert to data table format
mysubset<-roostusedata[reader_year== "2013"]

DT<-data.table(mysubset)


#for each indivdiual, count the number of occurrences of each zone, then extract the maximum
zoneuselist<-DT %>% group_by(pit) %>% dplyr::count(.,zone) %>% dplyr::mutate(maxzone=max(n)) %>% dplyr::ungroup() %>% dplyr::filter(n==maxzone)

#check for duplicate PITs
zoneuselist$check<-data.table(duplicated(zoneuselist$pit)) #those that are tied between areas will appear in both 


#create list of pit IDs for each zone
VCzoneuse<-filter(zoneuselist, zone=="vc")
HQzoneuse<-filter(zoneuselist, zone=="hq")
HBzoneuse<-filter(zoneuselist, zone=="fh")

#number of rows of each of these should sum to the total number of individuals unless there is a duplicate PIT - the difference will tell you how many individuals appear in multiple groups
nrow(VCzoneuse)+nrow(HQzoneuse)+nrow(HBzoneuse)

#get id of bat that appears twice
idcheck<-Reduce(intersect, list(VCzoneuse$pit, HQzoneuse$pit))

#extract relevant observations for each zone based on pit (just because of most used zone does not mean observations at other boxes will be omitted)
VCsnp<-DT[DT$pit %in% VCzoneuse$pit]
HQsnp<-DT[DT$pit %in% HQzoneuse$pit]
HBsnp<-DT[DT$pit %in% HBzoneuse$pit]

#write RData files as needed
save(VCsnp, file="VCsnp2019.RData")
save(HQsnp, file="HQsnp2019.RData")
save(HBsnp, file="HBsnp2019.RData")

```


For either the full year network, or for year-roosting area subset, run the following code chunk to test the network against the naive null model. This code also includes what is needed to calculate Rcom (community assortativity) as adapted from Shizuka and Farine 2016

```{r}
##for SRI and R
library(asnipe)
library(igraph)
library(data.table)
library(assortnet)
library(spatsoc)
library(rgexf)

#for Rcom
library(dplyr)
library(tidyr)

#optional
library(beepr) #code can take a while depending on the size of the network so this provides auditory notification at completion

library(vkR)

#run everything below for each location/year combination of interest to test Q, cvSRI and Rcom. 
#Change which network (year/loc combination) is being tested using the first code chunk in this document


##########Calculate SRI and Q for actual and random distribution###########
load("HQsnp2014.RData")
mysubset<-HQsnp

mysubset$daygroup<-paste(mysubset$box, "-", mysubset$reader_date)

individuals <- mysubset[, .(ID = pit, group = daygroup, day = reader_pitdt)]
sampleGBI <- get_group_by_individual(individuals, data_format = "individuals")

#bootstrap new networks avoiding network_permutation function 
mysubset[, reader_pitdt := as.POSIXct(reader_pitdt)]
group_times(mysubset, datetime = 'reader_pitdt', threshold = '1 day') #group times and shuffle box location within a day

mysubset$box1 <- as.numeric(mysubset$box)
mysubset$box2 <- as.numeric(mysubset$box)
group_pts(mysubset, threshold = 1, id = 'pit', coords = c('box1', 'box2'),
          timegroup = 'timegroup')

N = 1000 #SET NUMBER OF ITERATIONS HERE
rand <- randomizations(mysubset, type = 'daily', coords = NULL,
                       id = 'pit', group = 'group', 
                       datetime = 'reader_pitdt', iterations = N)

## count number of individuals in each iteration and location
rand[, uniqueN(pit), by = .(iteration)]
rand[, uniqueN(randomID), by = .(iteration)]

rand[, uniqueN(group), by = .(iteration)]

## Create a data.table of unique combinations of iteration, year exluding observed rows
iterLocLs <- unique(rand[!(observed), .(iteration)])

## Generate group by individual matrix 
# for each combination of iteration number and year
# 'group' generated by spatsoc::group_pts
# 'randomID' used instead of observed ID (type = 'step')
gbiLs <- mapply(
  FUN = function(i ) {
    get_gbi(rand[iteration == i ],
            'group', 'randomID')
  },
  i = iterLocLs$iter,
  
  SIMPLIFY = FALSE
)

## Generate a list of random networks
## 
netLs <- lapply(gbiLs, FUN = get_network,
                data_format = "GBI", association_index = "SRI")


## Generate graph and calculate network metrics for each random network
mets <- lapply(seq_along(netLs), function(n) {
  g <- graph.adjacency(netLs[[n]], 'undirected', 
                       diag = FALSE, weighted = TRUE)
  
  data.table(
    Q = modularity(cluster_fast_greedy(g)),
    iteration = iterLocLs$iter[[n]]
    
    
  )
})

## generate data.table from list output 
randomvalues<-rbindlist(mets)

#calculate actual Q value
individuals <- data.frame(ID=c(mysubset$pit),group=c(mysubset$daygroup),day=c(mysubset$reader_date))
sampleGBI <- get_group_by_individual(individuals, data_format = "individuals")
net_matrix <- get_network(sampleGBI,data_format="GBI",association_index="SRI")
graph_object <- graph.adjacency(net_matrix,mode="undirected",diag=FALSE,weighted=TRUE)

Qact<-modularity(cluster_fast_greedy(graph_object))

#compare actual Q value to distribution of random Q values to generate p value
x<-sum(randomvalues$Q >= Qact)
pQ<-x/length(randomvalues$Q)
pQ


#calculate cvSRI for each random network
SRI<-lapply(netLs, FUN=mean)
SRI2<-as.data.table(SRI)

sdran<-lapply(netLs, FUN=sd)
sdran2<-as.data.table(sdran)

cvran<-(sdran2/SRI2)*100
cvranfinal<-t(cvran)

#calculate cvSRI of actual network
actualmean<-mean(net_matrix)
sdactual<-sd(net_matrix)

#calculate CV for SRI
#note, sd = mean of the squared differences which is why this value can be greater than 1
cvactual<-(sd(net_matrix)/mean(net_matrix))*100

#compare actual CV SRI to distribution of random values to generate p value
x<-sum(cvranfinal >= cvactual)
pSRI<-x/length(cvranfinal)
pSRI


####Rcom code using appropriate bootstrapping - from Shizuka and Farine 2016

#create table to store results
network.community <- matrix(0,ncol(sampleGBI),ncol(sampleGBI))
network.present <- matrix(0,ncol(sampleGBI),ncol(sampleGBI))

# 1. Calculate network
network <- get_network(sampleGBI, data_format="GBI", association_index="SRI")

# 2. Calculate community membership of the observed network
community.observed <- fastgreedy.community(graph.adjacency(network, mode="undirected",weighted=TRUE))

#bootstrap new networks avoiding network_permutation function - SET # OF ITERATIONS HERE
for (i in 1:500) { #adjust this value to desired number of bootstraps 
  gbi.boot <- sampleGBI[sample(1:nrow(sampleGBI),nrow(sampleGBI),replace=TRUE),]
  network.boot <- get_network(gbi.boot,data_format="GBI", association_index="SRI")
  
  # This step calculates the community membership from the bootstrapped network
  community.boot <- fastgreedy.community(graph.adjacency(network.boot,mode="undirected",weighted=TRUE))
  
  # This step adds 1 to any dyads in the same community
  network.community <- network.community + outer(community.boot$membership, community.boot$membership,"==")
  
  
  # This step adds 1 to any dyads that are both present (in this case if they have at least 1 edge)
  network.present <- network.present + outer((rowSums(network.boot)>0),(rowSums(network.boot)>0),"*")
} 
#end boot strap	

# Calculate proportion of times observed in the same community
P <- network.community/network.present
P[!is.finite(P)] <- 0

# Calculate assortment from known community membership
rc <- assortment.discrete(P,community.observed$membership)$r

#get number of communities reported
unique(membership(community.observed))
#notification of completion
beep(sound=2)

```


For either the full year network, or for year-roosting area subset, run the following code chunk to test the network against the roost fidelity corrected null model 

```{r}
library(dplyr)
library(tidyr)
library(spatsoc)
library(asnipe)
library(igraph)
library(data.table)
library(beepr)

#This code is dependent on consistent ordering of observations to ensure that the randomized dates line up with the appropriate individual
mysubset<-roostusedata[reader_year== "2014"]

mysubset<-mysubset[order(mysubset, pit, reader_date)]


#this is what was used previously to create a "group" based on which box a bat was in on a given day. 
mysubset$daygroup<-paste(mysubset$box, "-", mysubset$reader_date)

#generate group by individual matrix for eventual calculation of actual network 
individualsActual <- mysubset[, .(ID = pit, group = daygroup, day = reader_date)]
sampleGBIActual <- get_group_by_individual(individualsActual, data_format = "individuals")


#run permutations with the above set how and desired number of permutations

##create list to store GBIs from random networks 
gbiLSRF<-list()

#changing group = daygroup to group=box 
individuals <- mysubset[, .(ID = pit, group = box, day = reader_date)]

#change date format 
individuals$day<-as.Date(individuals$day, format="%Y-%m-%d")

#convert data frame so that each individual is a list item
split_individuals<-split(individuals, individuals$ID)
#calls all days within each individual (list item)
actualdates<-lapply(split_individuals, `[[`, "day")

for (i in 1:1000) {
  
  #shuffle dates within each list item
  randomdates<-lapply(actualdates, sample, replace=FALSE)
  #unlist and add to individuals data frame
  individuals$day.new<-unlist(randomdates)
  #restore date format
  individuals$day.new<-as.Date(individuals$day.new, origin="1970-01-01")
  
  #once that is done, re-paste box and day to create the daygroup column in this new random network, and create the necessary GBI
  individualsRandom<-individuals
  individualsRandom$daygroup<-paste(individuals$group, "-", individuals$day.new)
  individualsRandom$group<-individualsRandom$daygroup
  individualsRandom<-select(individualsRandom, ID, group, day.new)
  sampleGBIrandom <- get_group_by_individual(individualsRandom, data_format = "individuals")
  
  #send sampleGBIrandom to gbiLS
  gbiLSRF[[i]]<-sampleGBIrandom
  
  
  
}
  
  
  ## From list of random GBIs, generate a list of random networks
  ## 
  netLsRF <- lapply(gbiLSRF, FUN = get_network,
                  data_format = "GBI", association_index = "SRI")
  
  #extract desired test statistics values from each list item 
  
  ## Generate graph and calculate network metrics
  metsRF <- lapply(seq_along(netLsRF), function(n) {
    g <- graph.adjacency(netLsRF[[n]], 'undirected', 
                         diag = FALSE, weighted = TRUE)
    
    data.table(
      Q = modularity(cluster_fast_greedy(g))
      
      
    )
  })
  
  
 ## generate data.table from list output 
  randomvaluesRF<-rbindlist(metsRF)
  
  #calculate actual Q value
  individualsActual <- mysubset[, .(ID = pit, group = daygroup, day = reader_date)]
  sampleGBIActual <- get_group_by_individual(individualsActual, data_format = "individuals")
  net_matrix <- get_network(sampleGBIActual,data_format="GBI",association_index="SRI")
  graph_object <- graph.adjacency(net_matrix,mode="undirected",diag=FALSE,weighted=TRUE)
  Qact<-modularity(cluster_fast_greedy(graph_object))
  
  #compare to distribution of random values to generate p value
  x<-sum(randomvaluesRF$Q >= Qact)
  pQ<-x/length(randomvaluesRF$Q)
  pQ
  

  
#calculate cvSRI of each random network
  SRI<-lapply(netLsRF, FUN=mean)
  SRI2<-as.data.table(SRI)
  
  sdran<-lapply(netLsRF, FUN=sd)
  sdran2<-as.data.table(sdran)
  
  cvran<-(sdran2/SRI2)*100
  cvranfinal<-t(cvran)
  
  #calcualte actual CV SRI value
  actualmean<-mean(net_matrix)
  sdactual<-sd(net_matrix)
  
  #calculate CV for SRI
  #note, sd = mean of the squared differences which is why this value can be greater than 1
  cvactual<-(sd(net_matrix)/mean(net_matrix))*100
  
  #compare actual to random to obtain p value
  x<-sum(cvranfinal >= cvactual)
  pSRI<-x/length(cvranfinal)
  pSRI
  
  #notification of completion
 beep(sound=2) 

```



For either full year or year-roosting area subset networks, this code creates outputs to be used with Gephi 0.9.2 to create network models (Figure 3 inset and Figure 4)

```{r}
library(asnipe)
library(igraph)
library(data.table)
library(tibble)


load("VCsnp2017.RData")
mysubset<-VCsnp
#merge box and pitymd to get daygroup
mysubset$daygroup<-paste(mysubset$box, "-", mysubset$reader_date)


#generate network 
individuals <- mysubset[, .(ID = pit, group = daygroup, day = reader_date)]
sampleGBI <- get_group_by_individual(individuals, data_format = "individuals")
net_matrix <- get_network(sampleGBI,data_format="GBI",association_index="SRI")
graph_object <- graph.adjacency(net_matrix,mode="undirected",diag=FALSE,weighted=TRUE)

#get community assignments 
community.observed <- fastgreedy.community(graph.adjacency(net_matrix, mode="undirected",weighted=TRUE))


#create a list of community membership and turn this into a table
groups<-as.list(membership(community.observed))
groups2<-as.data.frame(groups, check.names=FALSE) %>% t() %>% as.data.frame() %>% rownames_to_column(var="id")
groups2$groups<-groups2$V1
groups2$id<-as.character(groups2$id)

#this will be the "nodes table" used to assign community (colour) in Gephi
write.csv(groups2, file= "groupsVC2017.csv")

#output net_matrix and this will be loaded as the "matrix" to generate the network in Gephi
write.csv(net_matrix, file="matrixVC2017.csv")
```

For either full year networks or roosting area networks, run the following code to calculate roost use similarity between individuals and conduct a Mantel test to compare roost use similarity to the SRI
```{r}
library(data.table)
library(dplyr)
library(ecodist)
library(asnipe)
library(plyr)
library(tidyr)
library(proxy)


#create individual x individual matrix based on SRI
##great boxgroup column - 
mysubset$daygroup<-paste(mysubset$box, "-", mysubset$reader_date)

individuals<-data.frame(ID=c(mysubset$pit), group=c(mysubset$daygroup), day=c(mysubset$reader_date))
GBI<-get_group_by_individual(individuals, data_format="individuals")
association_matrix<-get_network(GBI, data_format="GBI", association_index="SRI")

#create individual x individual matrix based on roost use similarity
###for each pit, count number of occurrences in each box
roostuse<-mysubset %>% group_by(pit) %>% dplyr::count(box, name="use") %>% ungroup()
### sum number of occurrences for each pit to convert to proportion in each box
roostusetotal<-mysubset %>% dplyr::count(pit, name="total")

roostuse2<-join(roostuse, roostusetotal, by="pit")

roostuse2$proportion<-roostuse2$use/roostuse2$total

#drop the columns
roostuse2<-subset(roostuse2, select=-c(use, total))

###convert to individual x box matrix and populate with 0's

usematrix<-as.matrix(pivot_wider(roostuse2, names_from = box, values_from = proportion))

#set rownames to pit tags
rownames(usematrix) <- usematrix[,1]
#drop pit row
usematrix<-usematrix[,-1]
#setNA to zero
usematrix[is.na(usematrix)] <- 0

#calculate similarity in roost use to create individual x individual matrix for roost use
### since we are doing this there is no need to add rows for boxes that are not used

usesimilarity<-as.matrix(simil(usematrix, y = NULL, method = "correlation", diag = FALSE, upper = FALSE,
                               pairwise = FALSE, by_rows = TRUE, convert_distances = TRUE,
                               auto_convert_data_frames = TRUE))

usesimilarity=usesimilarity[order(rownames(usesimilarity)),order(rownames(usesimilarity))] #reorder the rows/columns by alphanumeric order
association_matrix=association_matrix[order(rownames(association_matrix)),order(rownames(association_matrix))] 

#run Mantel test to test correlation between use matrices - mrank=TRUE so ranked(Spearman method) since data are skewed towards 0
##make sure matrices are ordered the same 
#since expect positive correlation, use pval2
mantel(lower(usesimilarity)~ lower(association_matrix), nperm = 10000,
       mrank = TRUE, nboot = 500, pboot = 0.9, cboot = 0.95)

hist(usesimilarity)
hist(association_matrix)


#plot
plot(usesimilarity[upper.tri(usesimilarity)], association_matrix[upper.tri(association_matrix)], pch=19, col=rgb(red=0, green=0, blue=0, alpha=0.25), cex=1, xlab="Roost Fidelity Similarity", ylab="SRI", xlim=c(-0.5, 1), ylim=c(0,1))
```

